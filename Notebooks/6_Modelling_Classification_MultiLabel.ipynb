{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/luferIPCA/MIA-MLA-24-25/blob/main/6_Modelling_Classification_MultiLabel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"c1kf9qddTM4D"},"source":["# Masters' in Applied Artificial Intelligence\n","## Machine Learning Algorithms Course\n","\n","Notebooks for the MLA course\n","\n","by [*lufer*](mailto:lufer@ipca.pt)\n","\n","vers(3.0)\n","\n","---\n","(2025)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3yc6mD0jVeWN"},"source":["# ML Modelling - Part VI - Multi Label Classification Problems\n","\\\n","**Contents**:\n","\n","1.  **Create a Multi Label Classification ML Model**\n","2.  **L....***\n","\n"]},{"cell_type":"markdown","source":["This notebook explores the creation of Machine Learning models for Multi Label Classification Supervised Learning."],"metadata":{"id":"iAEg0vfdoiFr"}},{"cell_type":"markdown","metadata":{"id":"GP-NymupVL02"},"source":["# Environment preparation\n"]},{"cell_type":"markdown","metadata":{"id":"-3Rm857IVoPe"},"source":["**Importing necessary Libraries**"]},{"cell_type":"code","source":["\n","!pip install scikit-multilearn"],"metadata":{"id":"RI2YJzPzyyuT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vA1MzNI4TU_q"},"outputs":[],"source":["from skmultilearn.adapt import MLARAM #Adapted algotihm: Multi-label ARAM model\n","#see http://scikit.ml/api/skmultilearn.adapt.mlaram.html\n","\n","from skmultilearn.problem_transform import BinaryRelevance, ClassifierChain, LabelPowerset #Transformation to be used\n","\n","from sklearn.svm import SVC   #model to be used\n","from sklearn.model_selection import train_test_split  #prepare the dataset\n","from sklearn.metrics import hamming_loss              #meetric to be used\n","import pandas as pd\n","import scipy\n"]},{"cell_type":"markdown","metadata":{"id":"bDLxcgMwJEYA"},"source":["**Mounting Drive**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qxFY0ypTJJK9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e1d9a57d-834d-48d9-ee50-1e054ecba29c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gDrive/\n"]}],"source":["\n","from google.colab import drive\n","\n","# it will ask for your google drive credentiaals\n","drive.mount('/content/gDrive/', force_remount=True)"]},{"cell_type":"markdown","source":["#1 - Notes about Classification\n","\n","**Note 1:**\n","\n","Classification problems are different than regression problems primarily in their outputs. Classification problems involve categorizing data into discrete classes or labels, such as “spam” or “not spam” in email filtering models. In contrast,regression problems predict continuous, numerical outputs, like orecasting sales or temperatures.\n","\n","**Note 2:**\n","\n","\n","**Classification:**\n","*   Simple\n","*   Multi-Class\n","*   Multi-Label\n","\n","Where:\n","\n","* In Simple Classification, the predicted value can be one of the two existing classes;\n","\n","* Int he Mult-Class Classification Problems, the predicted can be one of the many existing classes.\n","\n","* In Multi Label Classification Problems the predicted value can be a combination of all existing labels, i.e., each instance (input) can be assigned multiple labels instead of just one."],"metadata":{"id":"b3koRS-wr8sq"}},{"cell_type":"markdown","source":["# 2 - Multi-Class Classification\n","\n","The typical example of Iris Dataset, using the LinearSVC model.\n","\n","**Steps:**\n","1. Load the Iris dataset.\n","2. Split dataset into training and testing sets.\n","3. Train the LinearSVC model.\n","4. Evaluate\n"],"metadata":{"id":"r7IPv79fsYfs"}},{"cell_type":"code","source":["from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = datasets.load_iris()\n","X, y = iris.data, iris.target  # Features and Labels\n","\n","# Split into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize features (always important for SVM performance)\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Train Linear SVC model for multi-class classification\n","model = LinearSVC(random_state=42, dual=False)  # dual=False is recommended when n_samples > n_features\n","model.fit(X_train, y_train)\n","\n","# Predict on test data\n","y_pred = model.predict(X_test)\n","\n","# Evaluate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Model Accuracy: {accuracy:.2f}\")\n","\n","#Note:\n","#In the primal formulation of linear SVC (i.e dual = False ), the optimisation variable is\n","#of dimension n_features. Whereas in the dual formulation (i.e dual = True ), the variable is of\n","#dimension n_samples.\n","#More importantly, the dual formulation requires the computation of an n_samplesxn_samples matrix.\n","#For this reason, when n_samples > n_features it is better to use dual = False\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q53SByOXsfJ7","outputId":"0ad44487-f53c-4d7e-83da-7022fb0103b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 1.00\n"]}]},{"cell_type":"markdown","source":["Try to compare with others multi-class classification model, like:\n","\n","* Randon Forest Classifier\n","* SVC (Support Vector Classifier) with Kernels\n","* Logistic Regression (Multi-class)\n","* K-Nearest Neighbors (KNN)\n","* Others"],"metadata":{"id":"CNon8CMNtOlf"}},{"cell_type":"markdown","source":["### 2.1 - Logist Regression with OvR\n","\n","OvR | OoR: One versus the Rest | One over the Rest\n","\n","Each class is predicted agains the others combined!"],"metadata":{"id":"8bQCnIp13MbM"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"metadata":{"id":"X_fWaSh83ZTn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.datasets import make_classification\n","from sklearn.metrics import accuracy_score,classification_report,confusion_matrix"],"metadata":{"id":"7pZQi5rb3e0s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## create the dummy dataset with 3 categories\n","X, y = make_classification(n_samples=1000, n_features=10,n_informative=3, n_classes=3, random_state=15)"],"metadata":{"id":"uiMAv5Oy3oTi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=42)"],"metadata":{"id":"zsSYPXst3wAf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Explore one-versus-rest\n","from sklearn.linear_model import LogisticRegression\n","logistic=LogisticRegression(multi_class='ovr')\n","logistic.fit(X_train,y_train)\n","y_pred=logistic.predict(X_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EAHE16Ta3znK","outputId":"1f4f733c-2438-41c6-ace6-a4aabddf8db3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["y_pred"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c-fRuZim3_aS","outputId":"b6bcf7e0-91d0-4210-aa2d-d90722103253"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 1, 2, 1, 1, 0, 0, 0, 2, 0, 2, 1, 2, 2, 2, 2, 2, 0, 0, 2, 2, 1,\n","       1, 1, 1, 0, 0, 0, 2, 1, 0, 2, 2, 1, 2, 0, 0, 2, 2, 1, 2, 2, 2, 1,\n","       2, 0, 1, 2, 0, 1, 0, 0, 0, 1, 1, 2, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n","       2, 1, 0, 1, 0, 1, 2, 1, 2, 2, 1, 0, 1, 0, 1, 0, 1, 2, 2, 0, 1, 2,\n","       2, 1, 1, 2, 2, 0, 0, 0, 2, 2, 0, 1, 2, 1, 2, 1, 0, 2, 0, 2, 0, 1,\n","       2, 1, 2, 2, 1, 1, 1, 1, 2, 0, 2, 0, 1, 2, 0, 0, 2, 2, 2, 1, 2, 0,\n","       2, 2, 0, 0, 0, 2, 0, 2, 0, 1, 2, 1, 1, 2, 0, 0, 1, 1, 2, 2, 2, 1,\n","       2, 0, 2, 2, 2, 1, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 2, 0, 1, 1, 1, 1,\n","       0, 2, 1, 0, 0, 1, 2, 2, 2, 2, 2, 0, 1, 1, 2, 2, 1, 2, 2, 2, 2, 1,\n","       0, 0, 1, 2, 2, 0, 0, 2, 1, 2, 1, 0, 0, 2, 1, 1, 1, 2, 2, 1, 2, 1,\n","       0, 1, 0, 0, 1, 0, 2, 1, 0, 2, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0, 0,\n","       0, 0, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 1, 2, 2, 2, 1, 0, 0, 1, 0, 2,\n","       1, 2, 0, 0, 0, 2, 2, 1, 2, 0, 1, 1, 0, 0, 0, 1, 0, 2, 2, 0, 2, 0,\n","       0, 0, 1, 1, 2, 0, 1, 2, 2, 0, 1, 2, 0, 2])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["score=accuracy_score(y_pred,y_test)\n","print(score)\n","print(classification_report(y_pred,y_test))\n","print(confusion_matrix(y_pred,y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FRfuzS454CC2","outputId":"8a255e0c-87e6-4a74-e923-6a08d5119bd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.79\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.87      0.84        97\n","           1       0.73      0.81      0.77        91\n","           2       0.82      0.71      0.76       112\n","\n","    accuracy                           0.79       300\n","   macro avg       0.79      0.79      0.79       300\n","weighted avg       0.79      0.79      0.79       300\n","\n","[[84  3 10]\n"," [10 74  7]\n"," [ 8 25 79]]\n"]}]},{"cell_type":"markdown","source":["# 3 - Multi Label Classification Problems\n"],"metadata":{"id":"W-9uny3ilsmg"}},{"cell_type":"markdown","source":["\n","\n","\n","**Strategies for supporting:**\n","\n","*  Problem Transformation\n","*  Adapting Algorithms\n","\n"],"metadata":{"id":"Yhz9QWwupiCe"}},{"cell_type":"markdown","source":["## 2.1 - Dataset Preparation\n","\n"],"metadata":{"id":"VFv4l95wtjKT"}},{"cell_type":"markdown","source":["### *Download Dataset*\n","\n","This dataset has data about music classification according to several (77) audio technical attributes\n"],"metadata":{"id":"9z_3Z0WVoiV6"}},{"cell_type":"code","source":["#Importing a real world dataset preparaed for Regression\n","\n","filePath='/content/gDrive/MyDrive/Colab Notebooks/MIA - ML - 2024-2025/Datasets/'\n","pd.set_option(\"display.precision\", 2)\n","music = pd.read_csv(filePath+\"Musica.csv\")\n","music.shape"],"metadata":{"id":"APRXNHdxoiV6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["music.head()\n"],"metadata":{"id":"lL8HmCOzyWgx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We want to use the influencing attributes (72) to predict the target 6 categories (type of music)."],"metadata":{"id":"9tQNAw7noiV6"}},{"cell_type":"code","source":["len(music)\n","#answer: more than 50"],"metadata":{"id":"EtNCUKN0zE3Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Understanding the Dataset"],"metadata":{"id":"hb1hcfpQzdoq"}},{"cell_type":"code","source":["labelClassses = music.iloc[:,0:6].values\n","attributes = music.iloc[:,7:78].values\n","#attributes\n","#DlabelClassses"],"metadata":{"id":"QU5_9PDN1W86"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Class labels : type of musics\n","music.columns[:6]"],"metadata":{"id":"qgC17L672Zjj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Influencing Attributes\n","music.columns[7:78]"],"metadata":{"id":"Uzum63k13HoC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Analysing initial dataset profile"],"metadata":{"id":"iQZ8JEtw3q3x"}},{"cell_type":"code","source":["#! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip"],"metadata":{"id":"6BZVGmz90Rcd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from pandas_profiling import ProfileReport"],"metadata":{"id":"XY-G_poD23mI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#profile = ProfileReport(music, title=\"Music Types\", html={'style' : {'full_width':True}})\n","#send result to file\n","#profile.to_file(output_file=filePath+\"MusicTypes.html\")\n","#ATTENTION: it takes to long..."],"metadata":{"id":"ygk2ZT5B46S_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Prepare the trainning dataset"],"metadata":{"id":"t7HftZF732Jn"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(attributes, labelClassses, test_size=0.3, random_state=0)\n"],"metadata":{"id":"sx-DWYiW39Q1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.2 - Strategy A - Adapting Algoritm\n","\n","\n","We are going to explore the Multi-label ARAM model.\n","\n","MLARAM - Machine Learning Adaptive Resonance Associative Map\n","\n",">Is a neural network model based on *Adaptive Resonance Theory (ART)*. It is used for incremental learning, pattern recognition, and classification tasks.\n","\n","\n",">MLARAM is great for adaptive, real-time learning.\n","\n","See: http://scikit.ml/api/skmultilearn.adapt.mlaram.html"],"metadata":{"id":"-ePtb9BQ1dGP"}},{"cell_type":"code","source":["import numpy as np\n","import scipy\n","\n","scipy.ones = np.ones    #scipy.ones was deprecated"],"metadata":{"id":"6omMDgZDAhX8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create an instance of the MLARAM model\n","ann = MLARAM()\n","ann.fit(X_train, y_train)"],"metadata":{"id":"4Da_gqJl8M3c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#make predictions\n","pred1 = ann.predict(X_test)\n","#evaluate\n","print(f\"Hamming Loss:  {hamming_loss(y_test,pred1)}\")\n","#Harmming Loss is a Lost Function =>t he lower value, the best performance"],"metadata":{"id":"K3mqLUqd9Ze9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.3 - Strategy B: Problem Transformation\n","\n","* In all cases we'll explore the SVC (Support Vector Classifier) as base classification model."],"metadata":{"id":"V1mKTZ_0fwkv"}},{"cell_type":"markdown","source":["### **2.3.1 - Using Binary Relevance**\n","\n","* Binary Relevance explore an individual binary classification between attributes and each class.\n","\n","* It prepares the data (transform the data) to be used by a normal classification model.\n"],"metadata":{"id":"ztHw8B8ggDkh"}},{"cell_type":"code","source":["# Binary Relevance with SVC\n","# Define a base classifier\n","binary = BinaryRelevance(classifier=SVC())\n","binary.fit(X_train, y_train)    #train\n","pred2 = binary.predict(X_test)  #predict\n","print(f\"Hamming Loss:  {hamming_loss(y_test,pred2)}\") #evaluate\n","#better performance?\n","#Yes! indeed the HL is lower!"],"metadata":{"id":"tD4pEtItA4wX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **2.3.1 - Using Classifier Chain**\n","\n","* Ensemble method used for multi-label classification, where labels are not independent but may have dependencies.\n","\n","* It also prepares the data (transform the data) to be used by a normal classification model.\n","\n","* Transforms multi-label classification into a sequence of binary classification problems:\n","  * Train one classifier per label.\n","  * Pass the predictions of previous classifiers as additional features to the next classifiers.\n"],"metadata":{"id":"7TRLeXMmiq7t"}},{"cell_type":"code","source":["# Create Classifier Chain using SVC as base classifier\n","chain = ClassifierChain(classifier=SVC())\n","#train the model\n","chain.fit(X_train, y_train)\n","#predict\n","pred3 = chain.predict(X_test)\n","#evaluate\n","print(f\"Hamming Loss:  {hamming_loss(y_test,pred3)}\")"],"metadata":{"id":"DxQ5_I6LBNg5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#H2\n","#using Randon Forest base model\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.multioutput import ClassifierChain\n","# Define a base classifier (e.g., Random Forest)\n","base_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","# Create Classifier Chain\n","chain_model = ClassifierChain(base_model, order='random')\n","# Train the model\n","chain_model.fit(X_train, y_train)\n","\n","# Predict\n","pred4 = chain_model.predict(X_test)\n","\n","# Convert sparse matrix to NumPy array (if needed)\n","if scipy.sparse.issparse(pred4):\n","    pred4 = pred4.toarray()\n","\n","#Note:\n","#y_test is a binary indicator matrix (only 0s and 1s).\n","# pred4 is not in binary format—it contains continuous values (probabilities) instead of 0s and 1s.\n","# this happens because RandomForestClassifier outputs probabilities rather than hard label assignments.\n","# Convert predictions to binary format (0s and 1s)\n","pred4 = (pred4 >= 0.5).astype(int)  # Apply threshold at 0.5\n","\n","# Compute Hamming Loss\n","hl = hamming_loss(y_test, pred4)\n","\n","# Print first 5 predictions\n","# print(pred4[:5])\n","\n","print(f\"Hamming Loss: {hl:.4f}\")\n","\n","# ATTENTION:\n","# Even the HL is lower (performed better) it should not be compared, because the base model are different\n","# (SVC and RF)"],"metadata":{"id":"0RLXUT5ylWis"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **2.3.1 - Using Label Powerset**\n","\n","* Create a new class label for any common sequence of used classes\n","* Transforms the problem into a multi-class classification problem\n","* Instead of treating each label separately (like Binary Relevance) or chaining them (like Classifier Chain), Label Powerset treats each unique combination of labels as a single class.\n","\n",">Works well when label dependencies are important\n","\n",">More efficient than training separate classifiers for each label\n","\n",">Not good for large numbers of labels (too many unique combinations → too many classes)"],"metadata":{"id":"f_KKgRSc1lZQ"}},{"cell_type":"code","source":["#create instance of LabelPowerset, using SVC as base model\n","label = LabelPowerset(classifier = SVC())\n","#train\n","label.fit(X_train, y_train)\n","#predict\n","pred5 = label.predict(X_test)\n","#evaluate\n","print(f\"Hamming Loss: {hamming_loss(y_test, pred5):.4f}\")"],"metadata":{"id":"eGvExW4lBVS0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **2.4 - Comparing all strategies**"],"metadata":{"id":"C_1Zvpth4YJo"}},{"cell_type":"code","source":["#Display all Hamming Loss results\n","\n","data = {\n","    \"MLARM\": hamming_loss(y_test,pred1),\n","    \"Binary Relevance\": hamming_loss(y_test,pred2),\n","    \"Classifier Chain\": hamming_loss(y_test,pred3),\n","    \"Label Powerset\": hamming_loss(y_test,pred4),\n","    }\n","\n","# Convert dictionary to DataFrame\n","df = pd.DataFrame(data, index=[\"All Hamming Loss\"])\n","\n","# Display DataFrame\n","#print(df)\n","\n","from IPython.display import display\n","display(df)"],"metadata":{"id":"ms8JnEps8aEw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.5 - Final Remarks\n","\n","* Use Classifier Chain if labels are dependent but you want a simple approach.\n","* Use Label Powerset if you strongly believe in label dependencies and have fewer unique label combinations.\n","* Use Binary Relevance for fast training when labels are mostly independent."],"metadata":{"id":"daKcvE8M4rw5"}},{"cell_type":"markdown","source":["Explore the books:\n","*   *Machine Learning*, Tom M. Mitchel\n","*   *Mastering Machine Learning with Python in Six Steps*, M\n","Manohar Swamynathan"],"metadata":{"id":"iJinjQFt4_-y"}},{"cell_type":"markdown","source":["## 2.6 - Exercise"],"metadata":{"id":"SkZ-14d55v8y"}},{"cell_type":"markdown","source":["1. Implement a \"pipeline\" to process all the three alternatives, Binary Relevance (BR), Classifier Chain (CC) and Label Powerset, using SVC as the base model.\n","2. Computes Hamming Loss & Accuracy for comparison\n","3. Displays results in a pandas DataFrame (formatted as a table)\n","4. Try the same, but using Randon Forest as the base model\n"],"metadata":{"id":"keZ0Nho_533U"}},{"cell_type":"code","source":["#Solution here\n","#use the same previous splits: X_train, X_test, y_train, y_test\n"],"metadata":{"id":"S6Q9zSLy6Ob9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#End!"],"metadata":{"id":"QjQdyenSYfpv"},"execution_count":null,"outputs":[]}]}